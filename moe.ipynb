{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669ff0b-2870-46af-9d34-6ad417ce47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# MH-MoE | 100 k samples | 50 epochs | combined loss/accuracy plot\n",
    "\n",
    "# %% imports & setup\n",
    "%matplotlib inline\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np, random, math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
    "\n",
    "# ---------- synthetic dataset ----------\n",
    "def make_data(n=100_000, d=20, n_classes=4):\n",
    "    X = torch.randn(n, d)\n",
    "    mtype = torch.randint(0, n_classes, (n, 1)).float()\n",
    "    logit = (\n",
    "        1.2*X[:,0] - .8*X[:,5] + .6*X[:,11] - 1.4*X[:,17]\n",
    "        + .9*mtype.squeeze()\n",
    "    )\n",
    "    y = torch.bernoulli(torch.sigmoid(logit)).unsqueeze(1)\n",
    "    X = torch.cat([X, mtype], dim=1)\n",
    "    return X, y\n",
    "\n",
    "# ---------- MH-MoE ----------\n",
    "class MHMoE(nn.Module):\n",
    "    def __init__(self, in_dim, n_experts=5, hidden=128):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(), nn.Linear(64, n_experts)\n",
    "        )\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden//2), nn.ReLU(),\n",
    "                nn.Linear(hidden//2, 1)\n",
    "            ) for _ in range(n_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = torch.softmax(self.gate(x), dim=1)              # [B,E]\n",
    "        logits = torch.cat([exp(x) for exp in self.experts], dim=1)\n",
    "        return (w * logits).sum(1, keepdim=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "@torch.no_grad()\n",
    "def val_step(model, dl):\n",
    "    model.eval()\n",
    "    tot, tot_loss, correct = 0, 0.0, 0\n",
    "    for xb, yb in dl:\n",
    "        logits = model(xb)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, yb, reduction=\"sum\")\n",
    "        probs = torch.sigmoid(logits)\n",
    "        tot_loss += loss.item()\n",
    "        correct  += ((probs > .5) == yb).sum().item()\n",
    "        tot      += yb.numel()\n",
    "    return tot_loss / tot, correct / tot\n",
    "\n",
    "# ---------- training loop ----------\n",
    "def run_demo(epochs=50, batch=4096, lr=1e-3, gamma=.3, step=25):\n",
    "    X, y = make_data()\n",
    "    ds = TensorDataset(X, y)\n",
    "    n_train = int(.8 * len(ds))\n",
    "    train_ds, val_ds = random_split(ds, [n_train, len(ds)-n_train])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=batch)\n",
    "\n",
    "    model = MHMoE(in_dim=X.shape[1])\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=step, gamma=gamma)\n",
    "\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        running, seen = 0.0, 0\n",
    "        for xb, yb in tqdm(train_dl, leave=False):\n",
    "            opt.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(model(xb), yb)\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "            running += loss.item() * yb.size(0)\n",
    "            seen    += yb.size(0)\n",
    "        train_losses.append(running / seen)\n",
    "\n",
    "        v_loss, v_acc = val_step(model, val_dl)\n",
    "        val_losses.append(v_loss); val_accs.append(v_acc)\n",
    "        sched.step()\n",
    "\n",
    "    # ---------- plot ----------\n",
    "    fig, ax1 = plt.subplots(figsize=(7,4))\n",
    "    ax1.plot(train_losses, label=\"train loss\", color=\"tab:blue\")\n",
    "    ax1.plot(val_losses,   label=\"val loss\",   color=\"tab:orange\")\n",
    "    ax1.set_xlabel(\"epoch\"); ax1.set_ylabel(\"BCE loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(val_accs, label=\"val accuracy\", color=\"tab:green\")\n",
    "    ax2.set_ylabel(\"accuracy\")\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    plt.title(\"MH-MoE | loss & accuracy\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "    print(f\"Validation â€” acc {val_accs[len(val_accs)-1]:.3f} \")\n",
    "\n",
    "run_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
